seed_everything: 1

data:
  class_path: src.gm_dataset.GMDataModule
  init_args:
    cfg_path: src/GraphMatching/experiments/spair.json
    num_keypoints: 6
    eval_samples: 256
    eval_on_test: False
    train_eval_samples: 128
    eval_batch_size: 64
    num_workers:  4
    train_batch_size: 8
    train_epoch_iters: 400
    test_samples: -1


model:
  class_path: src.models.GMModel
  init_args:
    core:
      class_path: src.CombOptNet.src.cores.SolverCore
      init_args:
        encoder:
          class_path: src.CombOptNet.src.encoders.DisjointEncoder
          init_args:
            ab_encoders:
              - class_path: src.CombOptNet.src.encoders.StaticABEncoder
                init_args:
                  num_vars: 36
                  num_constrs: 12
                  batch_size_extractor: src.GraphMatching.utils.utils.get_batch_size
                  init_a:
                    class_path: torch.nn.init.uniform_
                    init_args:
                      a: -0.5
                      b: 0.5
                  init_r:
                    class_path: torch.nn.init.constant_
                    init_args:
                      val: 0.2
                  init_o:
                    class_path: torch.nn.init.uniform_
                    init_args:
                      a: 0.25
                      b: 0.75

            c_encoder:
              class_path: src.encoders.KeyPointMatchingCEncoder
              init_args:
                pretrained_path: runs/neurips/kpcamera/comboptnet_init_pretrain0/val-256/seed-1/kp4/version_0/checkpoints/best.ckpt
                prefix: 'core.encoder.c_encoder.net.' 
                backbone_params:
                  alpha: 1.0

        solver:
          class_path: src.CombOptNet.ext.wrap.CombOptNet
          init_args:
            lb: 0.0
            ub: 1.0
            tau: 0.5
            num_threads: 1
            #          class_path: src.CombOptNet.src.ilp.CombOptNet
            #          init_args:
            #            vtype: 'B'
            #            env:
            #              class_path: gurobi.Env
            #              init_args:
            #                params:
            #                  OutputFlag: 0
            #            num_workers: 10
            #            criterion:
            #              class_path: src.CombOptNet.src.ilp.ILPLoss
            #              init_args:
            #                tau: 0.5

        criterion:
          class_path: torch.nn.L1Loss

    solver:
      class_path: src.CombOptNet.src.ilp.CombOptNet
      init_args:
        vtype: 'B'
        env:
          class_path: gurobi.Env
          init_args:
            params:
              OutputFlag: 0
              TimeLimit: 10
        num_workers: 10

    optimizer:
      custom:
          #        '.*c_encoder.*': 
          #            lr: 2.5e-6
        '.*node_layers.*':
            lr: 2.5e-5
        '.*edge_layers.*':
            lr: 2.5e-5
        '.*final_layers.*':
            lr: 2.5e-5
      class_path: torch.optim.AdamW
      init_args:
        lr: 1.0e-3
        weight_decay: 0.0

    lr_scheduler:
      # class_path: torch.optim.lr_scheduler.CyclicLR
      # init_args:
      #   base_lr: 0.0
      #   max_lr: 1.0e-3
      #   step_size_up: 1000
      #   cycle_momentum: false
      # config:
      #   interval: step

      class_path: torch.optim.lr_scheduler.ExponentialLR
      #class_path: torch.optim.lr_scheduler.MultiStepLR
      init_args:
         #gamma: 0.5
         gamma: 1.0
         #milestones: [20,40,60,80,100] 


        #lr_schedules = {
        #"long_halving": (10, (2, 4, 6, 8, 10), 0.5),
        #"short_halving": (2, (1,), 0.5),
        #"long_nodrop": (10, (10,), 1.0),
        #"minirun": (1, (10,), 1.0),
        #}



      # class_path: torch.optim.lr_scheduler.ReduceLROnPlateau
      # init_args:
      #   mode: max
      #   factor: 0.3
      #   patience: 100
      #   min_lr: 0.000001
      # config:
      #   monitor: train/acc/bit_acc
      #   interval: step

#start_val_after_n_epochs: 3
skip_initial_validation: 1
trainer:
  max_epochs: 100
  callbacks:
    - class_path: src.callbacks.Timer
    - class_path: src.callbacks.CustomLogging
    - class_path: pytorch_lightning.callbacks.Timer
      init_args:
        duration: 01:00:00:00
        interval: step

    - class_path: pytorch_lightning.callbacks.EarlyStopping
      init_args:
        monitor: val/acc/num
        patience: 10
        mode: max
 
    - class_path: pytorch_lightning.callbacks.ModelSummary
      init_args:
        max_depth: -1
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        monitor: 'val/acc/num'
        mode: 'max'
        filename: best
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        save_on_train_epoch_end: True  
        save_last: True

  log_every_n_steps: 10
  check_val_every_n_epoch: 1
  logger:
    class_path: pytorch_lightning.loggers.TensorBoardLogger
    init_args:
      save_dir: runs/neurips/kpcamera/comboptnet_init_pretrain1/val-256/seed-1
      name: kp6
      default_hp_metric: false
  # gradient_clip_val: 0.1
  gpus: 1
  auto_select_gpus: true
  num_sanity_val_steps: 0
  reload_dataloaders_every_n_epochs: 0
